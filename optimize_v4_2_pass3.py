#!/usr/bin/env python3
"""
Pass 3: Final Aggressive Optimization
Target: Push from 20% to 35-40% total reduction
Focus: Remaining verbose sections, examples, repetitive instructions
"""

import re
from pathlib import Path

def optimize_pass3(input_file, output_file):
    """Third and final aggressive optimization pass"""

    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()

    original_size = len(content)
    original_lines = len(content.split('\n'))

    print(f"Pass 3 Starting: {original_lines:,} lines, {original_size:,} bytes")

    # Stage 1: Consolidate card information extraction section
    print("\n[Stage 1] Condensing card info extraction...")
    content = condense_card_info_extraction(content)

    # Stage 2: Reduce subset/insert detection verbosity
    print("[Stage 2] Streamlining subset detection...")
    content = streamline_subset_detection(content)

    # Stage 3: Consolidate rarity classification
    print("[Stage 3] Compacting rarity classification...")
    content = compact_rarity_classification(content)

    # Stage 4: Reduce orientation/directional accuracy section
    print("[Stage 4] Condensing orientation section...")
    content = condense_orientation_section(content)

    # Stage 5: Streamline execution contract
    print("[Stage 5] Streamlining execution contract...")
    content = streamline_execution_contract(content)

    # Stage 6: Remove remaining verbose examples and tables
    print("[Stage 6] Removing verbose examples...")
    content = remove_verbose_examples(content)

    # Stage 7: Final cleanup - aggressive whitespace and formatting
    print("[Stage 7] Final aggressive cleanup...")
    content = final_aggressive_cleanup(content)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

    final_size = len(content)
    final_lines = len(content.split('\n'))

    lines_saved = original_lines - final_lines
    bytes_saved = original_size - final_size
    percent_reduction = (bytes_saved / original_size) * 100

    print(f"\n{'='*60}")
    print(f"PASS 3 COMPLETE")
    print(f"{'='*60}")
    print(f"Before Pass 3: {original_lines:,} lines, {original_size:,} bytes")
    print(f"After Pass 3:  {final_lines:,} lines, {final_size:,} bytes")
    print(f"This Pass:     {lines_saved:,} lines ({percent_reduction:.1f}%)")
    print(f"{'='*60}")

def condense_card_info_extraction(content):
    """Reduce verbose card information extraction section"""

    # Consolidate the verbose card back text extraction instructions
    compact_card_back = """ðŸ†• **CARD BACK TEXT:** Extract 2-4 sentence descriptive paragraph (bio/highlights/context). Not just stats. Null if no text."""

    pattern = r'ðŸ†• \*\*CARD BACK TEXT EXTRACTION:\*\*.*?(?=ðŸš¨ \*\*CRITICAL: ALWAYS CHECK)'
    content = re.sub(pattern, compact_card_back + "\n\n", content, flags=re.DOTALL)

    # Consolidate card name vs subset clarification
    compact_clarification = """ðŸ†• **CARD NAME vs SUBSET:**
â€¢ **player_or_character**: Athlete/character name
â€¢ **card_name**: Card title (e.g., "Round Numbers", "Gold Standard")
â€¢ **subset**: If card_name â‰  player name â†’ card_name likely IS the subset. Also check for parallel text."""

    pattern = r'ðŸ†• v4\.2 CARD NAME vs SUBSET CLARIFICATION:.*?(?=ðŸ†• SUBSET / INSERT DETECTION)'
    content = re.sub(pattern, compact_clarification + "\n\n", content, flags=re.DOTALL)

    return content

def streamline_subset_detection(content):
    """Consolidate verbose subset/insert detection instructions"""

    compact_subset = """ðŸ†• **SUBSET/INSERT DETECTION:**

**Check locations (SMALL PRINT):** Back of card (bottom/top/sides) â†’ Front borders â†’ Near card # â†’ Product info area

**Characteristics:** Small font, often includes "Insert"/"Parallel"/"Variant"/"Limited"
**Common patterns:** "Prizm Silver", "Mosaic Gold", "Refractor Wave", "Gold Standard", "Rookie Premiere"
**Don't confuse with:** Large decorative text, player/team names, generic phrases

**Steps:** (1) Check back small text (2) Check front borders (3) Check near card # (4) Check product info (5) Only null if ALL checked

**Examples:**
â€¢ Panini Prizm Silver: Back has "Silver Prizm Parallel" â†’ subset: "Silver Prizm"
â€¢ Topps Chrome Refractor: Side has "Refractor" â†’ subset: "Refractor"
â€¢ Base card: No subset text after checking all â†’ subset: null"""

    pattern = r'ðŸ†• SUBSET / INSERT DETECTION IMPROVEMENTS:.*?(?=ðŸ†• SERIAL NUMBER DETECTION)'
    content = re.sub(pattern, compact_subset + "\n\n", content, flags=re.DOTALL)

    return content

def compact_rarity_classification(content):
    """Consolidate rarity classification section"""

    compact_rarity = """â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RARITY & FEATURE CLASSIFICATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Assign highest tier visible:

1. 1-of-1 (Serial "1/1") | 2. SSP (/2-/25) | 3. SP (/26-/99) | 4. Authenticated Auto | 5. Memorabilia/Relic | 6. Modern Parallels (Prizm, Mosaic, Refractor, Chrome) | 7. Parallel/Insert Variant | 8. Rookie/Debut/1st Edition | 9. Limited Edition (/100-/999) | 10. Commemorative/Promo | 11. Base/Common

ðŸ†• **MODERN PARALLELS:** Prizm (Silver, Gold, Black, Ruby Wave, Tiger Stripe) | Refractor (X-Fractor, Wave, Atomic, Pulsar) | Mosaic (Silver, Gold, Stained Glass) | Chrome (Refractor, SuperFractor) | Special (Cracked Ice, Nebula, Galaxy)"""

    pattern = r'â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRARITY & FEATURE CLASSIFICATION\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€.*?(?=\nâ•+\n\[STEP 2\])'
    content = re.sub(pattern, compact_rarity + "\n\n", content, flags=re.DOTALL)

    return content

def condense_orientation_section(content):
    """Reduce orientation and directional accuracy verbosity"""

    compact_orientation = """ORIENTATION AND DIRECTIONAL ACCURACY:
â€¢ View images exactly as presented (no mental rotation/flip)
â€¢ Corners: viewer's perspective (Top Left = upper left as viewed)
â€¢ Portrait = taller than wide | Landscape = wider than tall
â€¢ Verify orientation via text/player position before describing defects"""

    pattern = r'ORIENTATION AND DIRECTIONAL ACCURACY:.*?(?=\nâ•+\nGRADING STEPS)'
    content = re.sub(pattern, compact_orientation + "\n\n", content, flags=re.DOTALL)

    return content

def streamline_execution_contract(content):
    """Consolidate execution contract section"""

    compact_contract = """â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXECUTION CONTRACT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You are a Professional Trading Card Grader performing precision visual evaluations.

**CRITICAL REQUIREMENTS:**
âœ… Complete ALL steps in order | Return valid JSON (response_format: json_object) | NEVER refuse (even slabbed cards) | Conservative defaults (lower grade, higher uncertainty) | Describe what you see, not expectations | Quantitative measurements with qualifiers ("approximately", "roughly")

**PATCHES APPLIED (v3.8):**
â€¢ PATCH 2: Front/Back centering independence | PATCH 6: Conservative rounding (.5 scores with uncertainty) | PATCH 3: Trimming requires compelling evidence both sides | PATCH 8: Mathematical validation | PATCH 10: Confidence letter consistency | WEAKEST LINK SCORING: Final grade = minimum of weighted category scores

**MANDATORY PRE-ANALYSIS:** Complete card info extraction BEFORE condition eval (foil/die-cut/relic affect interpretation)."""

    pattern = r'â•+\nEXECUTION CONTRACT\nâ•+.*?(?=\nâ•+\nGRADING STEPS)'
    content = re.sub(pattern, compact_contract + "\n\n", content, flags=re.DOTALL)

    return content

def remove_verbose_examples(content):
    """Remove overly verbose examples while keeping key ones"""

    # Remove verbose "REAL-WORLD EXAMPLES" from subset section (keep compact version from earlier)
    pattern = r'\*\*REAL-WORLD EXAMPLES:\*\*.*?(?=ðŸ†• SERIAL NUMBER DETECTION)'
    content = re.sub(pattern, "", content, flags=re.DOTALL)

    # Remove verbose "Example 1, Example 2, Example 3, Example 4" walkthroughs
    # Keep just the compact summary
    pattern = r'Example 1: \*\*Panini Prizm.*?Example 4: \*\*Base/Common Card\*\*.*?(?=ðŸ†• SERIAL NUMBER)'
    content = re.sub(pattern, "", content, flags=re.DOTALL)

    return content

def final_aggressive_cleanup(content):
    """Final aggressive whitespace and formatting reduction"""

    # Remove excessive blank lines (reduce to max 2)
    content = re.sub(r'\n{3,}', '\n\n', content)

    # Remove trailing whitespace on all lines
    lines = [line.rstrip() for line in content.split('\n')]
    content = '\n'.join(lines)

    # Reduce divider lengths
    content = re.sub(r'â”€{41,}', 'â”€'*40, content)
    content = re.sub(r'â•{71,}', 'â•'*70, content)

    # Remove repetitive "ðŸš¨ CRITICAL:" at start of many lines - keep first, consolidate
    # Count occurrences and reduce
    critical_count = content.count('ðŸš¨ CRITICAL:')
    if critical_count > 15:
        # Keep important ones, replace some with simpler markers
        content = content.replace('ðŸš¨ CRITICAL: Distinguish manufacturer', '**Distinguish manufacturer', 1)
        content = content.replace('ðŸš¨ CRITICAL: Carefully inspect BOTH', '**Inspect BOTH', 1)
        content = content.replace('ðŸš¨ CRITICAL: Any white fiber visible', '**White fiber rule:', 1)
        content = content.replace('ðŸš¨ CRITICAL: ANY white fleck visible', '**White fleck rule:', 1)

    # Remove excessive "ðŸ†•" markers - keep structural ones, remove inline ones
    content = re.sub(r'ðŸ†• \*\*CRITICAL RULE:', '**CRITICAL RULE:', content)
    content = re.sub(r'ðŸ†• \*\*DETAILED', '**DETAILED', content)

    # Consolidate repetitive "MANDATORY" warnings
    content = content.replace('ðŸš¨ **MANDATORY FIRST STEP', '**MANDATORY FIRST STEP', 1)
    content = content.replace('ðŸš¨ **MANDATORY:', '**MANDATORY:', 1)

    return content

if __name__ == "__main__":
    input_file = Path("prompts/conversational_grading_v4_2_ENHANCED_STRICTNESS.txt")
    output_file = Path("prompts/conversational_grading_v4_2_ENHANCED_STRICTNESS.txt")

    print("="*60)
    print("PASS 3: FINAL AGGRESSIVE OPTIMIZATION")
    print("="*60)

    optimize_pass3(input_file, output_file)
    print("\nPass 3 complete!")
